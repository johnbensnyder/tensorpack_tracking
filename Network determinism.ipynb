{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the process of identifying and tracking the values of tensors in a given network with an example using Mask RCNN.\n",
    "\n",
    "In order to run this notebook on EC2, ssh into your instance with the command\n",
    "\n",
    "```ssh -i /your/ec2/keypair -L localhost:8888:localhost:8888 -L localhost:6006:localhost:6006 ec2-user@ip\n",
    "nohup jupyter notebook --no-browser --ip=0.0.0.0 > notebook.log &\n",
    "tensorboard -logdir ~/logs\n",
    "```\n",
    "\n",
    "This notebook is broken into _ sections. First, we generate a small dataset consisting of a single image from the coco data. We then look at how to track the tensors within Mask RCNN using that image. Finally, we track the gradients that backpropogate through the network.\n",
    "\n",
    "### Generate a single image dataset\n",
    "\n",
    "Mask RCNN isn't setup to deal with a single image, so we instead create a small dataset that's just the same image repeated 10 times.\n",
    "\n",
    "Randomly select an image from the COCO data, and move it to a new directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import copy \n",
    "\n",
    "DATA_DIR = \"/home/ec2-user/data/\"\n",
    "SINGLE_DATA = \"/home/ec2-user/single_data/\"\n",
    "COUNT = 10\n",
    "\n",
    "shutil.rmtree(SINGLE_DATA, ignore_errors=True)\n",
    "\n",
    "os.makedirs(SINGLE_DATA, exist_ok=True)\n",
    "os.makedirs(os.path.join(SINGLE_DATA, \"train2017\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(SINGLE_DATA, \"annotations\"), exist_ok=True)\n",
    "\n",
    "image_dir = os.path.join(DATA_DIR, \"train2017\")\n",
    "\n",
    "image = np.random.choice(os.listdir(image_dir), size=1)[0]\n",
    "\n",
    "for num in range(1, COUNT+1):\n",
    "    shutil.copy(os.path.join(image_dir, image), os.path.join(SINGLE_DATA, \"train2017\", \n",
    "                                                             image.split('.')[0]+str(num)+'.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0cff0eaeb8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(plt.imread(os.path.join(SINGLE_DATA, \n",
    "                        \"train2017\", \n",
    "                        np.random.choice(os.listdir(os.path.join(SINGLE_DATA, \"train2017\"))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the labels and mask annotations from the COCO data. We can just grab all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/single_data/val2017'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_dir = os.path.join(DATA_DIR, \"annotations\")\n",
    "'''\n",
    "shutil.copy(os.path.join(annotation_dir, 'instances_val2017.json'),\n",
    "            os.path.join(SINGLE_DATA, \"annotations\", \"instances_val2017.json\"))\n",
    "\n",
    "shutil.copy(os.path.join(annotation_dir, 'captions_val2017.json'),\n",
    "            os.path.join(SINGLE_DATA, \"annotations\", \"captions_val2017.json\"))\n",
    "\n",
    "shutil.copy(os.path.join(annotation_dir, 'person_keypoints_val2017.json'),\n",
    "            os.path.join(SINGLE_DATA, \"annotations\", \"person_keypoints_val2017.json\"))'''\n",
    "\n",
    "shutil.copytree(os.path.join(DATA_DIR, 'val2017'),\n",
    "            os.path.join(SINGLE_DATA, \"val2017\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new small set of annotations that match the filenames for the single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(annotation_dir, 'instances_train2017.json')) as infile:\n",
    "    instances = json.loads(infile.readline())\n",
    "    \n",
    "image_info = [i for i in instances['images'] if i['file_name'] == image]\n",
    "image_ids = [i['id'] for i in image_info]\n",
    "instances['annotations'] = [i for i in instances['annotations'] if i['image_id'] in image_ids]\n",
    "instances['images'] = [i for i in instances['images'] if i['id'] in image_ids]\n",
    "\n",
    "images = []\n",
    "annotations = []\n",
    "for i in range(COUNT):\n",
    "    images.append(copy(instances['images'][0]))\n",
    "    images[i]['file_name'] = image.split('.')[0]+str(i+1) + '.jpg'\n",
    "    images[i]['id'] = int(str(images[i]['id']) + str(i+1))\n",
    "    annotations.append(copy(instances['annotations'][0]))\n",
    "    annotations[i]['image_id'] = int(str(annotations[i]['image_id']) + str(i+1))\n",
    "\n",
    "instances['images'] = images\n",
    "instances['annotations'] = annotations\n",
    "\n",
    "\n",
    "with open(os.path.join(SINGLE_DATA, \"annotations\", \"instances_train2017.json\"), 'w') as outfile:\n",
    "    outfile.write(json.dumps(instances))\n",
    "    \n",
    "with open(os.path.join(annotation_dir, 'captions_train2017.json')) as infile:\n",
    "    captions = json.loads(infile.readline())\n",
    "\n",
    "captions['annotations'] = [i for i in captions['annotations'] if i['image_id'] in image_ids]\n",
    "captions['images'] = [i for i in captions['images'] if i['id'] in image_ids]\n",
    "\n",
    "images = []\n",
    "annotations = []\n",
    "for i, an_image in enumerate(images):\n",
    "    images.append(copy(captions['images'][0]))\n",
    "    images[i]['file_name'] = images[i]['file_name'].split('.')[0] + str(i+1) + '.jpg'\n",
    "    images[i]['id'] = int(str(images[i]['id']) + str(i+1))\n",
    "\n",
    "captions['images'] = images\n",
    "\n",
    "with open(os.path.join(SINGLE_DATA, \"annotations\", \"captions_train2017.json\"), 'w') as outfile:\n",
    "    outfile.write(json.dumps(captions))\n",
    "\n",
    "with open(os.path.join(annotation_dir, 'person_keypoints_train2017.json')) as infile:\n",
    "    keypoints = json.loads(infile.readline())\n",
    "\n",
    "keypoints['annotations'] = [i for i in keypoints['annotations'] if i['image_id'] in image_ids]\n",
    "keypoints['images'] = [i for i in keypoints['images'] if i['id'] in image_ids]\n",
    "\n",
    "images = [copy(keypoints['images'][0]) for _ in range(COUNT)]\n",
    "for i, an_image in enumerate(images):\n",
    "    images[i]['file_name'] = images[i]['file_name'].split('.')[0] + str(i+1) + '.jpg'\n",
    "    images[i]['id'] = int(str(images[i]['id']) + str(i+1))\n",
    "\n",
    "with open(os.path.join(SINGLE_DATA, \"annotations\", \"person_keypoints_train2017.json\"), 'w') as outfile:\n",
    "    outfile.write(json.dumps(keypoints))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in tensorpack packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1014 07:06:47.808326 139694008874816 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorpack-0.9.0.1-py3.6.egg/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "W1014 07:06:48.910633 139694008874816 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#os.environ['TF_CUDNN_DETERMINISTIC'] = 'true'\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import tensorpack.utils.viz as tpviz\n",
    "from tensorpack import *\n",
    "from tensorpack.tfutils.common import get_tf_version_tuple\n",
    "sys.path.append('/home/ec2-user/mask-rcnn-tensorflow/MaskRCNN')\n",
    "from model.generalized_rcnn import ResNetFPNModel\n",
    "from config import finalize_configs, config as cfg\n",
    "from eval import DetectionResult, predict_image, multithread_predict_dataflow, EvalCallback\n",
    "from performance import ThroughputTracker, humanize_float\n",
    "from data import get_eval_dataflow, get_train_dataflow, get_batch_train_dataflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = ResNetFPNModel(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg.DATA.BASEDIR = '/home/ec2-user/data'\n",
    "cfg.DATA.BASEDIR = '/home/ec2-user/single_data'\n",
    "#cfg.DATA.BASEDIR = '/home/ec2-user/small_data'\n",
    "cfg.BACKBONE.WEIGHTS = '/home/ec2-user/data/pretrained-models/ImageNet-R50-AlignPadding.npz'\n",
    "#cfg.MODE_FPN=True\n",
    "#cfg.FPN.NORM = 'GN'\n",
    "#cfg.TRAIN.BATCH_SIZE_PER_GPU = 4\n",
    "tf.set_random_seed(cfg.TRAIN.SEED)\n",
    "fix_rng_seed(cfg.TRAIN.SEED)\n",
    "np.random.seed(cfg.TRAIN.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train dataflow\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[1014 07:06:49 @dataset.py:50]\u001b[0m Instances loaded from /home/ec2-user/single_data/annotations/instances_train2017.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9006.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:49 @timer.py:50]\u001b[0m Load annotations for train2017 finished, time:0.0030sec.\n",
      "Done loading roidbs\n",
      "\u001b[32m[1014 07:06:49 @data.py:509]\u001b[0m Filtered 0 images which contain no non-crowd groudtruth boxes. Total #images for training: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batching roidbs\n",
      "Done batching roidbs\n",
      "\u001b[32m[1014 07:06:49 @config.py:280]\u001b[0m Config: ------------------------------------------\n",
      "{'BACKBONE': {'FREEZE_AFFINE': False,\n",
      "              'FREEZE_AT': 2,\n",
      "              'NORM': 'FreezeBN',\n",
      "              'RESNET_NUM_BLOCKS': [3, 4, 6, 3],\n",
      "              'STRIDE_1X1': False,\n",
      "              'TF_PAD_MODE': False,\n",
      "              'WEIGHTS': '/home/ec2-user/data/pretrained-models/ImageNet-R50-AlignPadding.npz'},\n",
      " 'DATA': {'BASEDIR': '/home/ec2-user/single_data',\n",
      "          'CLASS_NAMES': ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
      "                          'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
      "                          'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
      "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n",
      "                          'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
      "                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
      "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
      "                          'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\n",
      "                          'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant',\n",
      "                          'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
      "                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
      "                          'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
      "                          'hair drier', 'toothbrush'],\n",
      "          'NUM_CATEGORY': 80,\n",
      "          'NUM_CLASS': 81,\n",
      "          'TRAIN': ['train2017'],\n",
      "          'VAL': ('val2017',)},\n",
      " 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),\n",
      "         'BOXCLASS_CONV_HEAD_DIM': 256,\n",
      "         'BOXCLASS_FC_HEAD_DIM': 1024,\n",
      "         'BOXCLASS_HEAD_FUNC': 'boxclass_2fc_head',\n",
      "         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',\n",
      "         'NORM': 'None',\n",
      "         'NUM_CHANNEL': 256,\n",
      "         'PROPOSAL_MODE': 'Level',\n",
      "         'RESOLUTION_REQUIREMENT': 32},\n",
      " 'FRCNN': {'BATCH_PER_IM': 512,\n",
      "           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],\n",
      "           'FG_RATIO': 0.25,\n",
      "           'FG_THRESH': 0.5},\n",
      " 'MODE_FPN': True,\n",
      " 'MODE_MASK': True,\n",
      " 'MRCNN': {'HEAD_DIM': 256},\n",
      " 'PREPROC': {'MAX_SIZE': 1344.0,\n",
      "             'PADDING_SHAPES': [(800, 1000), (800, 1200), (800, 1350)],\n",
      "             'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
      "             'PIXEL_STD': [58.395, 57.12, 57.375],\n",
      "             'PREDEFINED_PADDING': False,\n",
      "             'TEST_SHORT_EDGE_SIZE': 800,\n",
      "             'TRAIN_SHORT_EDGE_SIZE': [800, 800]},\n",
      " 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),\n",
      "         'ANCHOR_SIZES': (32, 64, 128, 256, 512),\n",
      "         'ANCHOR_STRIDE': 16,\n",
      "         'BATCH_PER_IM': 256,\n",
      "         'CROWD_OVERLAP_THRESH': 9.99,\n",
      "         'FG_RATIO': 0.5,\n",
      "         'HEAD_DIM': 1024,\n",
      "         'MIN_SIZE': 0.1,\n",
      "         'NEGATIVE_ANCHOR_THRESH': 0.3,\n",
      "         'NUM_ANCHOR': 15,\n",
      "         'POSITIVE_ANCHOR_THRESH': 0.7,\n",
      "         'PROPOSAL_NMS_THRESH': 0.7,\n",
      "         'SLOW_ACCURATE_MASK': True,\n",
      "         'TEST_PER_LEVEL_NMS_TOPK': 1000,\n",
      "         'TEST_POST_NMS_TOPK': 1000,\n",
      "         'TEST_PRE_NMS_TOPK': 6000,\n",
      "         'TOPK_PER_IMAGE': True,\n",
      "         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,\n",
      "         'TRAIN_POST_NMS_TOPK': 2000,\n",
      "         'TRAIN_PRE_NMS_TOPK': 12000,\n",
      "         'UNQUANTIZED_ANCHOR': True},\n",
      " 'TEST': {'FRCNN_NMS_THRESH': 0.5,\n",
      "          'RESULTS_PER_IM': 100,\n",
      "          'RESULT_SCORE_THRESH': 0.05,\n",
      "          'RESULT_SCORE_THRESH_VIS': 0.3},\n",
      " 'TRAIN': {'BASE_LR': 0.00125,\n",
      "           'BATCH_SIZE_PER_GPU': 1,\n",
      "           'DETERMINISTIC': False,\n",
      "           'EVAL_PERIOD': 25,\n",
      "           'GRADIENT_CLIP': 0,\n",
      "           'LR_EPOCH_SCHEDULE': [(8, 0.1), (10, 0.01), (12, None)],\n",
      "           'NCHW': True,\n",
      "           'NUM_GPUS': 1,\n",
      "           'SEED': 1234,\n",
      "           'STARTING_EPOCH': 1,\n",
      "           'WARMUP_INIT_LR': 0.00041250000000000005,\n",
      "           'WARMUP_STEPS': 1000,\n",
      "           'WEIGHT_DECAY': 0.0001},\n",
      " 'TRAINER': 'replicated'}\n"
     ]
    }
   ],
   "source": [
    "train_dataflow = get_batch_train_dataflow(cfg.TRAIN.BATCH_SIZE_PER_GPU)\n",
    "finalize_configs(is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:49.936255 139694008874816 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorpack-0.9.0.1-py3.6.egg/tensorpack/tfutils/sessinit.py:259: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_init = get_model_loader(cfg.BACKBONE.WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincfg = TrainConfig(\n",
    "            model=MODEL,\n",
    "            data=QueueInput(train_dataflow),\n",
    "            steps_per_epoch=1,\n",
    "            max_epoch=1,\n",
    "            session_init=session_init,\n",
    "            session_config=None,\n",
    "            starting_epoch=cfg.TRAIN.STARTING_EPOCH\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SimpleTrainer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have our data and the model is setup. In order to export a gradient from the graph, we need to add a print statement to the tensor we want to track. For example, say we want to export the second to last layer of the backbone network (c4). Add this import to the top of the backbone.py file:\n",
    "\n",
    "```from performance import print_runtime_tensor, print_runtime_tensor_loose_branch```\n",
    "\n",
    "The, just after the c4 tensor is created in the network, add this line:\n",
    "\n",
    "```c4 = print_runtime_tensor(\"tensor_c4_forward\", c4)```\n",
    "\n",
    "Similarly, say we want to output a list of tensors. Perhaps the full output of the backbone (p23456). We can use something like:\n",
    "\n",
    "```p23456 = [print_runtime_tensor(\"tensor_p23456_{}_forward\".format(i), j) for i,j in enumerate(p23456)]```\n",
    "\n",
    "On the other hand, we might want to see a tensor that isn't actually used later in the graph, which means it wouldn't normally execute such that we can output it. This can be dome using the \n",
    "\n",
    "```print_runtime_tensor_loose_branch```\n",
    "\n",
    "For this, you need a downstream trigger tesnor to force the print of the tensor of interest. Say we have a tensor `t5` that isn't used in the graph, but `t1` is. We can print `t5` with:\n",
    "\n",
    "```t1 = print_runtime_tensor_loose_branch(\"tensor_t5_forward\", t5, trigger_tensor=t1)```\n",
    "\n",
    "Finally, say we want to print the gradients of the backwards pass. This is a little more complicated. Add this gradient printer class to the generalized_rcnn.py file:\n",
    "\n",
    "```\n",
    "class GradientPrinter(tf.train.Optimizer):\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "    def compute_gradients(self, *args, **kwargs):\n",
    "        return self.opt.compute_gradients(*args, **kwargs)\n",
    "    def apply_gradients(self, gradvars, global_step=None, name=None):\n",
    "        old_grads, v = zip(*gradvars)\n",
    "        old_grads = [print_runtime_tensor(\"tensor_{}_backward\".format(i.name), j) for i,j in zip(v, old_grads)]\n",
    "        for i in v:\n",
    "            print(\"gradient_name: {}\".format(i.name))\n",
    "        gradvars = list(zip(old_grads, v))\n",
    "        return self.opt.apply_gradients(gradvars, global_step, name)\n",
    "```\n",
    "\n",
    "Inside the detection model class, modify the optimizer to pass through the gradient printer.\n",
    "\n",
    "```\n",
    "opt = tf.train.MomentumOptimizer(lr, 0.9)\n",
    "opt = GradientPrinter(opt)\n",
    "```\n",
    "\n",
    "Once the print function has been added, run the paragraph below with the capture magic function to catch the printed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:51 @input_source.py:222]\u001b[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:51.453391 139694008874816 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorpack-0.9.0.1-py3.6.egg/tensorpack/tfutils/summary.py:266: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:51 @trainers.py:49]\u001b[0m Building graph for a single training tower ...\n",
      "\u001b[32m[1014 07:06:51 @registry.py:127]\u001b[0m conv0 input: [None, 3, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:51.495776 139694008874816 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorpack-0.9.0.1-py3.6.egg/tensorpack/models/conv2d.py:81: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "W1014 07:06:53.834445 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c867fae10>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:53 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:53.839741 139694008874816 deprecation.py:506] From /home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:53 @registry.py:135]\u001b[0m conv0 output: [None, 64, None, None]\n",
      "\u001b[32m[1014 07:06:53 @registry.py:127]\u001b[0m pool0 input: [None, 64, None, None]\n",
      "\u001b[32m[1014 07:06:54 @registry.py:135]\u001b[0m pool0 output: [None, 64, None, None]\n",
      "\u001b[32m[1014 07:06:54 @registry.py:127]\u001b[0m group0/block0/conv1 input: [None, 64, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:54.340950 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c864f7f28>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:54 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:54 @registry.py:135]\u001b[0m group0/block0/conv1 output: [None, 64, None, None]\n",
      "\u001b[32m[1014 07:06:54 @registry.py:127]\u001b[0m group0/block0/conv2 input: [None, 64, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:54.634319 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c862b2d30>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:54 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:54 @registry.py:135]\u001b[0m group0/block0/conv2 output: [None, 64, None, None]\n",
      "\u001b[32m[1014 07:06:54 @registry.py:127]\u001b[0m group0/block0/conv3 input: [None, 64, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:55.039958 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c86072e48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:55 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:55 @registry.py:135]\u001b[0m group0/block0/conv3 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:06:55 @registry.py:127]\u001b[0m group0/block0/convshortcut input: [None, 64, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:55.336205 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c867a3080>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:55 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:55 @registry.py:135]\u001b[0m group0/block0/convshortcut output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:06:55 @registry.py:127]\u001b[0m group0/block1/conv1 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:55.634265 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c8624b668>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:55 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:55 @registry.py:135]\u001b[0m group0/block1/conv1 output: [None, 64, None, None]\n",
      "\u001b[32m[1014 07:06:55 @registry.py:127]\u001b[0m group0/block1/conv2 input: [None, 64, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:55.908733 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c85fc6828>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:55 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:55 @registry.py:135]\u001b[0m group0/block1/conv2 output: [None, 64, None, None]\n",
      "\u001b[32m[1014 07:06:55 @registry.py:127]\u001b[0m group0/block1/conv3 input: [None, 64, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:56.181242 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c85d80940>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:56 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:56 @registry.py:135]\u001b[0m group0/block1/conv3 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:06:56 @registry.py:127]\u001b[0m group0/block2/conv1 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:56.646521 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c85b2c668>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:56 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:56 @registry.py:135]\u001b[0m group0/block2/conv1 output: [None, 64, None, None]\n",
      "\u001b[32m[1014 07:06:56 @registry.py:127]\u001b[0m group0/block2/conv2 input: [None, 64, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:56.921034 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c8674c128>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:56 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:56 @registry.py:135]\u001b[0m group0/block2/conv2 output: [None, 64, None, None]\n",
      "\u001b[32m[1014 07:06:56 @registry.py:127]\u001b[0m group0/block2/conv3 input: [None, 64, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:57.220490 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c86351c50>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:57 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:57 @registry.py:135]\u001b[0m group0/block2/conv3 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:06:57 @registry.py:127]\u001b[0m group1/block0/conv1 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:57.513423 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c867dcc88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:57 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:57 @registry.py:135]\u001b[0m group1/block0/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1014 07:06:57 @registry.py:127]\u001b[0m group1/block0/conv2 input: [None, 128, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:57.832424 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c85ac0780>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:57 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:57 @registry.py:135]\u001b[0m group1/block0/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1014 07:06:57 @registry.py:127]\u001b[0m group1/block0/conv3 input: [None, 128, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:58.275435 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c858e4898>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:58 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:58 @registry.py:135]\u001b[0m group1/block0/conv3 output: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:06:58 @registry.py:127]\u001b[0m group1/block0/convshortcut input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:58.567104 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c865ca940>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:58 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:58 @registry.py:135]\u001b[0m group1/block0/convshortcut output: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:06:58 @registry.py:127]\u001b[0m group1/block1/conv1 input: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:58.885244 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c86799fd0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:58 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:58 @registry.py:135]\u001b[0m group1/block1/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1014 07:06:58 @registry.py:127]\u001b[0m group1/block1/conv2 input: [None, 128, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:59.186418 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c86514f98>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:59 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:59 @registry.py:135]\u001b[0m group1/block1/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1014 07:06:59 @registry.py:127]\u001b[0m group1/block1/conv3 input: [None, 128, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:59.477164 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c85704ef0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:59 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:59 @registry.py:135]\u001b[0m group1/block1/conv3 output: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:06:59 @registry.py:127]\u001b[0m group1/block2/conv1 input: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:06:59.902649 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c85598c50>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:06:59 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:06:59 @registry.py:135]\u001b[0m group1/block2/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1014 07:06:59 @registry.py:127]\u001b[0m group1/block2/conv2 input: [None, 128, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:00.191490 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c867f3f60>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:00 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:00 @registry.py:135]\u001b[0m group1/block2/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1014 07:07:00 @registry.py:127]\u001b[0m group1/block2/conv3 input: [None, 128, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:00.475469 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c86268da0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:00 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:00 @registry.py:135]\u001b[0m group1/block2/conv3 output: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:07:00 @registry.py:127]\u001b[0m group1/block3/conv1 input: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:00.761482 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c867fa1d0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:00 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:00 @registry.py:135]\u001b[0m group1/block3/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1014 07:07:00 @registry.py:127]\u001b[0m group1/block3/conv2 input: [None, 128, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:01.033855 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c8520d278>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:01 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:01 @registry.py:135]\u001b[0m group1/block3/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1014 07:07:01 @registry.py:127]\u001b[0m group1/block3/conv3 input: [None, 128, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:01.310705 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c84fcd390>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:01 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:01 @registry.py:135]\u001b[0m group1/block3/conv3 output: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:07:01 @registry.py:127]\u001b[0m group2/block0/conv1 input: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:01.763172 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c84d7a0f0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:01 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:01 @registry.py:135]\u001b[0m group2/block0/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:01 @registry.py:127]\u001b[0m group2/block0/conv2 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:02.052013 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c86218b38>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:02 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:02 @registry.py:135]\u001b[0m group2/block0/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:02 @registry.py:127]\u001b[0m group2/block0/conv3 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:02.393374 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c867f3e48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:02 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:02 @registry.py:135]\u001b[0m group2/block0/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1014 07:07:02 @registry.py:127]\u001b[0m group2/block0/convshortcut input: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:02.703028 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c8518bb70>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:02 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:02 @registry.py:135]\u001b[0m group2/block0/convshortcut output: [None, 1024, None, None]\n",
      "\u001b[32m[1014 07:07:02 @registry.py:127]\u001b[0m group2/block1/conv1 input: [None, 1024, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:03.021591 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c84c20780>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:03 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:03 @registry.py:135]\u001b[0m group2/block1/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:03 @registry.py:127]\u001b[0m group2/block1/conv2 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:03.293997 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c849de940>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:03 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:03 @registry.py:135]\u001b[0m group2/block1/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:03 @registry.py:127]\u001b[0m group2/block1/conv3 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:03.604079 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c84932c88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:03 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:03 @registry.py:135]\u001b[0m group2/block1/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1014 07:07:03 @registry.py:127]\u001b[0m group2/block2/conv1 input: [None, 1024, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:04.066955 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c8497d6d8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:04 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:04 @registry.py:135]\u001b[0m group2/block2/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:04 @registry.py:127]\u001b[0m group2/block2/conv2 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:04.362823 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c865ca550>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:04 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:04 @registry.py:135]\u001b[0m group2/block2/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:04 @registry.py:127]\u001b[0m group2/block2/conv3 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:04.652175 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c8530c630>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:04 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:04 @registry.py:135]\u001b[0m group2/block2/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1014 07:07:04 @registry.py:127]\u001b[0m group2/block3/conv1 input: [None, 1024, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:04.963691 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c84cf8128>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:04 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:05 @registry.py:135]\u001b[0m group2/block3/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:05 @registry.py:127]\u001b[0m group2/block3/conv2 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:05.251933 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c84bbd2e8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:05 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:05 @registry.py:135]\u001b[0m group2/block3/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:05 @registry.py:127]\u001b[0m group2/block3/conv3 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:05.535177 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c8440c400>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:05 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:05 @registry.py:135]\u001b[0m group2/block3/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1014 07:07:05 @registry.py:127]\u001b[0m group2/block4/conv1 input: [None, 1024, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:05.822114 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c841d5ba8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:05 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:05 @registry.py:135]\u001b[0m group2/block4/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:05 @registry.py:127]\u001b[0m group2/block4/conv2 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:06.310135 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c8444e5f8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:06 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:06 @registry.py:135]\u001b[0m group2/block4/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:06 @registry.py:127]\u001b[0m group2/block4/conv3 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:06.602008 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c84102978>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:06 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:06 @registry.py:135]\u001b[0m group2/block4/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1014 07:07:06 @registry.py:127]\u001b[0m group2/block5/conv1 input: [None, 1024, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:06.902186 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c85d7fc88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:06 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:06 @registry.py:135]\u001b[0m group2/block5/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:06 @registry.py:127]\u001b[0m group2/block5/conv2 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:07.205976 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c854840f0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:07 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:07 @registry.py:135]\u001b[0m group2/block5/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:07 @registry.py:127]\u001b[0m group2/block5/conv3 input: [None, 256, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:07.499483 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c844e1080>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:07 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:07 @registry.py:135]\u001b[0m group2/block5/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1014 07:07:07 @registry.py:127]\u001b[0m group3/block0/conv1 input: [None, 1024, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:07.777896 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c84614860>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:07 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:07 @registry.py:135]\u001b[0m group3/block0/conv1 output: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:07:07 @registry.py:127]\u001b[0m group3/block0/conv2 input: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:08.083839 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c7fd22cc0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:08 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:08 @registry.py:135]\u001b[0m group3/block0/conv2 output: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:07:08 @registry.py:127]\u001b[0m group3/block0/conv3 input: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:08.571669 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c7fae6e10>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:08 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:08 @registry.py:135]\u001b[0m group3/block0/conv3 output: [None, 2048, None, None]\n",
      "\u001b[32m[1014 07:07:08 @registry.py:127]\u001b[0m group3/block0/convshortcut input: [None, 1024, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:08.868495 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c845c7390>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:08 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:08 @registry.py:135]\u001b[0m group3/block0/convshortcut output: [None, 2048, None, None]\n",
      "\u001b[32m[1014 07:07:08 @registry.py:127]\u001b[0m group3/block1/conv1 input: [None, 2048, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:09.164727 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c866eb5c0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:09 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:09 @registry.py:135]\u001b[0m group3/block1/conv1 output: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:07:09 @registry.py:127]\u001b[0m group3/block1/conv2 input: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:09.451548 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c84210780>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:09 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:09 @registry.py:135]\u001b[0m group3/block1/conv2 output: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:07:09 @registry.py:127]\u001b[0m group3/block1/conv3 input: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:09.753614 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c85eb9668>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:09 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:09 @registry.py:135]\u001b[0m group3/block1/conv3 output: [None, 2048, None, None]\n",
      "\u001b[32m[1014 07:07:09 @registry.py:127]\u001b[0m group3/block2/conv1 input: [None, 2048, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:10.054990 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c84a202e8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:10 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:10 @registry.py:135]\u001b[0m group3/block2/conv1 output: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:127]\u001b[0m group3/block2/conv2 input: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:10.324989 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c7f8ce240>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:10 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:10 @registry.py:135]\u001b[0m group3/block2/conv2 output: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:127]\u001b[0m group3/block2/conv3 input: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:10.605622 139694008874816 ag_logging.py:145] Entity <function BatchNorm at 0x7f0cb1497840> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function BatchNorm at 0x7f0cb1497840>: AssertionError: If not matching a CFG node, must be a block statement: <gast.gast.ImportFrom object at 0x7f0c7f602358>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:10 @batch_norm.py:166]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1014 07:07:10 @registry.py:135]\u001b[0m group3/block2/conv3 output: [None, 2048, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:127]\u001b[0m fpn input: [None, 256, None, None],[None, 512, None, None],[None, 1024, None, None],[None, 2048, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:127]\u001b[0m fpn/lateral_1x1_c2 input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:135]\u001b[0m fpn/lateral_1x1_c2 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:127]\u001b[0m fpn/lateral_1x1_c3 input: [None, 512, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:135]\u001b[0m fpn/lateral_1x1_c3 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:127]\u001b[0m fpn/lateral_1x1_c4 input: [None, 1024, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:135]\u001b[0m fpn/lateral_1x1_c4 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:127]\u001b[0m fpn/lateral_1x1_c5 input: [None, 2048, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:135]\u001b[0m fpn/lateral_1x1_c5 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:127]\u001b[0m fpn/upsample_lat5 input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:135]\u001b[0m fpn/upsample_lat5 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:10 @registry.py:127]\u001b[0m fpn/upsample_lat4 input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m fpn/upsample_lat4 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:127]\u001b[0m fpn/upsample_lat3 input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m fpn/upsample_lat3 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p2 input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p2 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p3 input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p3 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p4 input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p4 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p5 input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p5 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:127]\u001b[0m fpn/maxpool_p6 input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m fpn/maxpool_p6 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m fpn output: [None, 256, None, None],[None, 256, None, None],[None, 256, None, None],[None, 256, None, None],[None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:127]\u001b[0m rpn input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:127]\u001b[0m rpn/conv0 input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m rpn/conv0 output: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:127]\u001b[0m rpn/class input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m rpn/class output: [None, 3, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:127]\u001b[0m rpn/box input: [None, 256, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m rpn/box output: [None, 12, None, None]\n",
      "\u001b[32m[1014 07:07:11 @registry.py:135]\u001b[0m rpn output: [None, None, None, 3],[None, 12, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:12.010782 139694008874816 deprecation.py:323] From /home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1014 07:07:12.094996 139694008874816 deprecation_wrapper.py:119] From /home/ec2-user/mask-rcnn-tensorflow/MaskRCNN/model/rpn.py:121: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:13 @registry.py:127]\u001b[0m fastrcnn input: [None, 256, 7, 7]\n",
      "\u001b[32m[1014 07:07:13 @registry.py:127]\u001b[0m fastrcnn/fc6 input: [None, 256, 7, 7]\n",
      "\u001b[32m[1014 07:07:13 @registry.py:135]\u001b[0m fastrcnn/fc6 output: [None, 1024]\n",
      "\u001b[32m[1014 07:07:13 @registry.py:127]\u001b[0m fastrcnn/fc7 input: [None, 1024]\n",
      "\u001b[32m[1014 07:07:13 @registry.py:135]\u001b[0m fastrcnn/fc7 output: [None, 1024]\n",
      "\u001b[32m[1014 07:07:13 @registry.py:135]\u001b[0m fastrcnn output: [None, 1024]\n",
      "\u001b[32m[1014 07:07:13 @registry.py:127]\u001b[0m fastrcnn/outputs input: [None, 1024]\n",
      "\u001b[32m[1014 07:07:13 @registry.py:127]\u001b[0m fastrcnn/outputs/class input: [None, 1024]\n",
      "\u001b[32m[1014 07:07:13 @registry.py:135]\u001b[0m fastrcnn/outputs/class output: [None, 81]\n",
      "\u001b[32m[1014 07:07:13 @registry.py:127]\u001b[0m fastrcnn/outputs/box input: [None, 1024]\n",
      "\u001b[32m[1014 07:07:13 @registry.py:135]\u001b[0m fastrcnn/outputs/box output: [None, 324]\n",
      "\u001b[32m[1014 07:07:13 @registry.py:135]\u001b[0m fastrcnn/outputs output: [None, 81],[None, 81, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:14 @registry.py:127]\u001b[0m maskrcnn input: [None, 256, 14, 14]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:127]\u001b[0m maskrcnn/fcn0 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:135]\u001b[0m maskrcnn/fcn0 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:127]\u001b[0m maskrcnn/fcn1 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:135]\u001b[0m maskrcnn/fcn1 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:127]\u001b[0m maskrcnn/fcn2 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:135]\u001b[0m maskrcnn/fcn2 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:127]\u001b[0m maskrcnn/fcn3 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:135]\u001b[0m maskrcnn/fcn3 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:127]\u001b[0m maskrcnn/deconv input: [None, 256, 14, 14]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:135]\u001b[0m maskrcnn/deconv output: [None, 256, 28, 28]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:127]\u001b[0m maskrcnn/conv input: [None, 256, 28, 28]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:135]\u001b[0m maskrcnn/conv output: [None, 80, 28, 28]\n",
      "\u001b[32m[1014 07:07:14 @registry.py:135]\u001b[0m maskrcnn output: [None, 80, 28, 28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:14.939120 139694008874816 deprecation.py:506] From /home/ec2-user/mask-rcnn-tensorflow/MaskRCNN/model_box.py:195: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:15 @regularize.py:97]\u001b[0m regularize_cost() found 21 variables to regularize.\n",
      "\u001b[32m[1014 07:07:15 @regularize.py:22]\u001b[0m The following tensors will be regularized: fpn/lateral_1x1_c2/W:0, fpn/lateral_1x1_c3/W:0, fpn/lateral_1x1_c4/W:0, fpn/lateral_1x1_c5/W:0, fpn/posthoc_3x3_p2/W:0, fpn/posthoc_3x3_p3/W:0, fpn/posthoc_3x3_p4/W:0, fpn/posthoc_3x3_p5/W:0, rpn/conv0/W:0, rpn/class/W:0, rpn/box/W:0, fastrcnn/fc6/W:0, fastrcnn/fc7/W:0, fastrcnn/outputs/class/W:0, fastrcnn/outputs/box/W:0, maskrcnn/fcn0/W:0, maskrcnn/fcn1/W:0, maskrcnn/fcn2/W:0, maskrcnn/fcn3/W:0, maskrcnn/deconv/W:0, maskrcnn/conv/W:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:18 @monitor.py:259]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m logger directory was not set. Ignore TFEventWriter.\n",
      "\u001b[32m[1014 07:07:18 @monitor.py:300]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m logger directory was not set. Ignore JSONWriter.\n",
      "\u001b[32m[1014 07:07:18 @model_utils.py:66]\u001b[0m \u001b[36mTrainable Variables: \n",
      "\u001b[0mname                        shape                   dim\n",
      "--------------------------  -----------------  --------\n",
      "fpn/lateral_1x1_c2/W:0      [1, 1, 256, 256]      65536\n",
      "fpn/lateral_1x1_c2/b:0      [256]                   256\n",
      "fpn/lateral_1x1_c3/W:0      [1, 1, 512, 256]     131072\n",
      "fpn/lateral_1x1_c3/b:0      [256]                   256\n",
      "fpn/lateral_1x1_c4/W:0      [1, 1, 1024, 256]    262144\n",
      "fpn/lateral_1x1_c4/b:0      [256]                   256\n",
      "fpn/lateral_1x1_c5/W:0      [1, 1, 2048, 256]    524288\n",
      "fpn/lateral_1x1_c5/b:0      [256]                   256\n",
      "fpn/posthoc_3x3_p2/W:0      [3, 3, 256, 256]     589824\n",
      "fpn/posthoc_3x3_p2/b:0      [256]                   256\n",
      "fpn/posthoc_3x3_p3/W:0      [3, 3, 256, 256]     589824\n",
      "fpn/posthoc_3x3_p3/b:0      [256]                   256\n",
      "fpn/posthoc_3x3_p4/W:0      [3, 3, 256, 256]     589824\n",
      "fpn/posthoc_3x3_p4/b:0      [256]                   256\n",
      "fpn/posthoc_3x3_p5/W:0      [3, 3, 256, 256]     589824\n",
      "fpn/posthoc_3x3_p5/b:0      [256]                   256\n",
      "rpn/conv0/W:0               [3, 3, 256, 256]     589824\n",
      "rpn/conv0/b:0               [256]                   256\n",
      "rpn/class/W:0               [1, 1, 256, 3]          768\n",
      "rpn/class/b:0               [3]                       3\n",
      "rpn/box/W:0                 [1, 1, 256, 12]        3072\n",
      "rpn/box/b:0                 [12]                     12\n",
      "fastrcnn/fc6/W:0            [12544, 1024]      12845056\n",
      "fastrcnn/fc6/b:0            [1024]                 1024\n",
      "fastrcnn/fc7/W:0            [1024, 1024]        1048576\n",
      "fastrcnn/fc7/b:0            [1024]                 1024\n",
      "fastrcnn/outputs/class/W:0  [1024, 81]            82944\n",
      "fastrcnn/outputs/class/b:0  [81]                     81\n",
      "fastrcnn/outputs/box/W:0    [1024, 324]          331776\n",
      "fastrcnn/outputs/box/b:0    [324]                   324\n",
      "maskrcnn/fcn0/W:0           [3, 3, 256, 256]     589824\n",
      "maskrcnn/fcn0/b:0           [256]                   256\n",
      "maskrcnn/fcn1/W:0           [3, 3, 256, 256]     589824\n",
      "maskrcnn/fcn1/b:0           [256]                   256\n",
      "maskrcnn/fcn2/W:0           [3, 3, 256, 256]     589824\n",
      "maskrcnn/fcn2/b:0           [256]                   256\n",
      "maskrcnn/fcn3/W:0           [3, 3, 256, 256]     589824\n",
      "maskrcnn/fcn3/b:0           [256]                   256\n",
      "maskrcnn/deconv/W:0         [2, 2, 256, 256]     262144\n",
      "maskrcnn/deconv/b:0         [256]                   256\n",
      "maskrcnn/conv/W:0           [1, 1, 256, 80]       20480\n",
      "maskrcnn/conv/b:0           [80]                     80\u001b[36m\n",
      "Total #vars=42, #params=20892404, size=79.70MB\u001b[0m\n",
      "\u001b[32m[1014 07:07:18 @base.py:210]\u001b[0m Setup callbacks graph ...\n",
      "\u001b[32m[1014 07:07:18 @argtools.py:148]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m \"import prctl\" failed! Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[1014 07:07:18 @summary.py:48]\u001b[0m [MovingAverageSummary] 27 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.\n",
      "\u001b[32m[1014 07:07:18 @summary.py:95]\u001b[0m Summarizing collection 'summaries' of size 30.\n",
      "\u001b[32m[1014 07:07:18 @base.py:231]\u001b[0m Creating the session ...\n",
      "\u001b[32m[1014 07:07:26 @base.py:237]\u001b[0m Initializing the session ...\n",
      "\u001b[32m[1014 07:07:26 @sessinit.py:206]\u001b[0m Variables to restore from dict: group2/block3/conv1/bn/mean/EMA:0, group3/block1/conv3/bn/gamma:0, group2/block2/conv1/bn/variance/EMA:0, group2/block0/conv3/bn/variance/EMA:0, group2/block2/conv3/W:0, group1/block1/conv1/bn/mean/EMA:0, group2/block0/conv3/W:0, group3/block0/conv1/bn/mean/EMA:0, group1/block3/conv3/W:0, group1/block0/convshortcut/bn/mean/EMA:0, conv0/bn/variance/EMA:0, group1/block0/conv1/bn/beta:0, group0/block2/conv1/bn/beta:0, group1/block1/conv3/W:0, group2/block2/conv1/bn/gamma:0, group0/block2/conv3/bn/variance/EMA:0, group2/block0/conv1/bn/variance/EMA:0, group3/block2/conv1/W:0, group2/block1/conv2/W:0, group1/block0/conv3/bn/variance/EMA:0, group2/block4/conv3/bn/gamma:0, group3/block0/conv2/bn/mean/EMA:0, group2/block2/conv3/bn/mean/EMA:0, group2/block5/conv2/bn/beta:0, group3/block2/conv3/bn/gamma:0, conv0/W:0, group1/block0/convshortcut/bn/gamma:0, group0/block0/conv1/W:0, group0/block2/conv1/bn/variance/EMA:0, group0/block0/conv2/bn/mean/EMA:0, group2/block4/conv1/bn/beta:0, group1/block0/conv2/bn/mean/EMA:0, group3/block2/conv2/bn/mean/EMA:0, group0/block1/conv3/W:0, group1/block1/conv2/bn/gamma:0, group2/block3/conv2/bn/mean/EMA:0, group3/block2/conv2/W:0, group2/block0/convshortcut/bn/beta:0, group1/block0/convshortcut/W:0, group1/block2/conv2/bn/gamma:0, group2/block5/conv2/bn/gamma:0, group0/block1/conv1/bn/mean/EMA:0, group3/block2/conv3/W:0, group0/block0/conv3/bn/variance/EMA:0, group3/block0/convshortcut/W:0, group1/block3/conv1/bn/mean/EMA:0, group1/block0/convshortcut/bn/variance/EMA:0, group3/block1/conv2/bn/gamma:0, group0/block1/conv2/bn/beta:0, group2/block1/conv2/bn/variance/EMA:0, group2/block3/conv2/bn/variance/EMA:0, group3/block2/conv3/bn/variance/EMA:0, group3/block0/conv2/bn/variance/EMA:0, group1/block0/convshortcut/bn/beta:0, group3/block1/conv1/bn/gamma:0, group0/block0/conv3/bn/mean/EMA:0, group0/block2/conv1/bn/gamma:0, group3/block0/conv1/bn/beta:0, group1/block2/conv2/W:0, group3/block2/conv1/bn/gamma:0, group3/block2/conv1/bn/mean/EMA:0, group0/block1/conv1/bn/beta:0, group2/block0/convshortcut/bn/gamma:0, group0/block1/conv2/bn/mean/EMA:0, group2/block2/conv2/bn/variance/EMA:0, group1/block0/conv3/bn/gamma:0, group3/block0/convshortcut/bn/beta:0, group2/block5/conv3/bn/variance/EMA:0, group2/block2/conv1/W:0, group2/block2/conv3/bn/variance/EMA:0, group2/block1/conv3/W:0, group1/block0/conv1/bn/gamma:0, group3/block0/conv2/bn/beta:0, group2/block5/conv3/bn/beta:0, group1/block3/conv1/W:0, group1/block2/conv3/bn/beta:0, group2/block1/conv1/bn/variance/EMA:0, group1/block0/conv3/bn/beta:0, group2/block3/conv2/bn/beta:0, group3/block0/convshortcut/bn/gamma:0, group0/block2/conv3/bn/gamma:0, group1/block0/conv1/bn/mean/EMA:0, group1/block2/conv1/bn/beta:0, group2/block4/conv1/bn/mean/EMA:0, group0/block0/conv3/W:0, group2/block0/conv2/bn/beta:0, group3/block0/conv3/bn/gamma:0, group3/block0/conv3/bn/variance/EMA:0, group2/block0/conv1/bn/beta:0, group1/block1/conv2/bn/beta:0, group0/block2/conv2/bn/mean/EMA:0, group0/block2/conv2/bn/beta:0, group1/block3/conv2/bn/variance/EMA:0, group1/block1/conv1/W:0, group2/block1/conv3/bn/beta:0, group0/block1/conv1/bn/variance/EMA:0, group3/block0/conv3/bn/mean/EMA:0, group3/block1/conv2/bn/beta:0, group3/block1/conv3/bn/beta:0, group0/block0/conv2/bn/beta:0, group2/block0/conv1/bn/mean/EMA:0, group2/block3/conv1/bn/variance/EMA:0, group3/block1/conv2/bn/mean/EMA:0, group3/block1/conv2/W:0, conv0/bn/gamma:0, group3/block1/conv1/bn/variance/EMA:0, group0/block1/conv1/bn/gamma:0, group2/block1/conv3/bn/mean/EMA:0, group2/block0/conv3/bn/gamma:0, group2/block0/convshortcut/bn/mean/EMA:0, group1/block2/conv3/W:0, group1/block2/conv3/bn/mean/EMA:0, group1/block3/conv2/bn/gamma:0, group2/block3/conv3/bn/mean/EMA:0, group1/block3/conv1/bn/variance/EMA:0, group3/block2/conv2/bn/gamma:0, group1/block0/conv3/bn/mean/EMA:0, group3/block1/conv3/W:0, group1/block2/conv1/bn/variance/EMA:0, group2/block5/conv1/bn/beta:0, conv0/bn/mean/EMA:0, group2/block4/conv3/W:0, group0/block0/convshortcut/bn/mean/EMA:0, group2/block5/conv2/W:0, group1/block1/conv2/bn/variance/EMA:0, group3/block0/conv2/W:0, group0/block1/conv3/bn/beta:0, group0/block2/conv1/bn/mean/EMA:0, group0/block0/conv3/bn/gamma:0, group0/block2/conv3/W:0, group3/block1/conv3/bn/variance/EMA:0, group2/block4/conv1/bn/variance/EMA:0, group1/block0/conv2/bn/beta:0, group1/block0/conv1/bn/variance/EMA:0, group3/block2/conv1/bn/beta:0, group1/block2/conv3/bn/variance/EMA:0, group1/block0/conv2/bn/gamma:0, group1/block0/conv2/bn/variance/EMA:0, group0/block0/conv1/bn/mean/EMA:0, group0/block0/convshortcut/bn/variance/EMA:0, group1/block3/conv1/bn/gamma:0, group2/block1/conv1/bn/mean/EMA:0, group2/block0/conv1/bn/gamma:0, group1/block2/conv1/W:0, group3/block1/conv3/bn/mean/EMA:0, group2/block1/conv1/bn/beta:0, group0/block0/convshortcut/bn/gamma:0, group3/block0/conv3/bn/beta:0, group2/block1/conv2/bn/gamma:0, group2/block3/conv3/bn/variance/EMA:0, group1/block3/conv2/bn/beta:0, group2/block4/conv2/bn/variance/EMA:0, group1/block2/conv2/bn/beta:0, group0/block1/conv3/bn/variance/EMA:0, group2/block5/conv3/bn/mean/EMA:0, group1/block1/conv2/bn/mean/EMA:0, group1/block0/conv3/W:0, group2/block0/conv2/W:0, group2/block3/conv3/bn/gamma:0, group3/block0/convshortcut/bn/variance/EMA:0, group3/block0/convshortcut/bn/mean/EMA:0, group1/block3/conv1/bn/beta:0, group2/block4/conv2/bn/gamma:0, group2/block5/conv3/W:0, group1/block1/conv3/bn/gamma:0, group3/block2/conv2/bn/beta:0, group2/block4/conv2/bn/mean/EMA:0, group3/block2/conv3/bn/mean/EMA:0, group1/block3/conv2/W:0, group2/block5/conv1/W:0, group1/block2/conv2/bn/variance/EMA:0, group2/block5/conv2/bn/mean/EMA:0, group0/block0/convshortcut/bn/beta:0, group2/block0/conv3/bn/mean/EMA:0, group2/block3/conv3/W:0, group0/block2/conv2/bn/variance/EMA:0, group0/block2/conv3/bn/beta:0, group3/block0/conv1/W:0, group2/block5/conv1/bn/gamma:0, group0/block2/conv2/W:0, group1/block1/conv3/bn/mean/EMA:0, group3/block2/conv1/bn/variance/EMA:0, group1/block3/conv3/bn/gamma:0, group2/block2/conv3/bn/beta:0, group2/block3/conv1/W:0, group2/block0/conv2/bn/variance/EMA:0, group2/block2/conv2/W:0, group0/block1/conv1/W:0, group2/block3/conv2/W:0, group1/block1/conv1/bn/variance/EMA:0, group0/block1/conv3/bn/mean/EMA:0, group1/block3/conv3/bn/variance/EMA:0, group2/block2/conv1/bn/mean/EMA:0, group2/block1/conv1/bn/gamma:0, group1/block1/conv1/bn/beta:0, group1/block3/conv2/bn/mean/EMA:0, group2/block2/conv3/bn/gamma:0, group2/block0/conv2/bn/mean/EMA:0, group2/block0/conv3/bn/beta:0, group3/block1/conv1/bn/beta:0, group3/block1/conv2/bn/variance/EMA:0, group0/block1/conv2/bn/variance/EMA:0, group0/block1/conv2/W:0, group2/block4/conv2/bn/beta:0, group2/block2/conv2/bn/mean/EMA:0, group0/block0/conv3/bn/beta:0, group1/block3/conv3/bn/beta:0, group0/block2/conv1/W:0, group3/block0/conv2/bn/gamma:0, group2/block0/conv1/W:0, group1/block3/conv3/bn/mean/EMA:0, conv0/bn/beta:0, group2/block4/conv3/bn/beta:0, group2/block3/conv3/bn/beta:0, group3/block2/conv2/bn/variance/EMA:0, group1/block1/conv3/bn/beta:0, group2/block1/conv3/bn/gamma:0, group2/block4/conv1/bn/gamma:0, group0/block0/conv1/bn/beta:0, group2/block4/conv1/W:0, group1/block2/conv3/bn/gamma:0, group3/block1/conv1/bn/mean/EMA:0, group3/block0/conv1/bn/gamma:0, group0/block0/conv1/bn/gamma:0, group2/block0/conv2/bn/gamma:0, group2/block1/conv1/W:0, group2/block1/conv2/bn/beta:0, group2/block2/conv1/bn/beta:0, group1/block1/conv3/bn/variance/EMA:0, group2/block1/conv3/bn/variance/EMA:0, group3/block0/conv1/bn/variance/EMA:0, group3/block2/conv3/bn/beta:0, group0/block1/conv3/bn/gamma:0, group1/block0/conv1/W:0, group2/block3/conv1/bn/beta:0, group0/block0/conv2/W:0, group2/block5/conv2/bn/variance/EMA:0, group0/block2/conv2/bn/gamma:0, group2/block4/conv2/W:0, group0/block0/convshortcut/W:0, group1/block2/conv1/bn/mean/EMA:0, group1/block1/conv2/W:0, group2/block2/conv2/bn/gamma:0, group2/block5/conv1/bn/mean/EMA:0, group1/block2/conv2/bn/mean/EMA:0, group2/block0/convshortcut/W:0, group1/block0/conv2/W:0, group2/block4/conv3/bn/variance/EMA:0, group3/block0/conv3/W:0, group0/block1/conv2/bn/gamma:0, group2/block3/conv2/bn/gamma:0, group1/block2/conv1/bn/gamma:0, group2/block4/conv3/bn/mean/EMA:0, group2/block0/convshortcut/bn/variance/EMA:0, group1/block1/conv1/bn/gamma:0, group3/block1/conv1/W:0, group2/block3/conv1/bn/gamma:0, group0/block0/conv2/bn/gamma:0, group2/block5/conv1/bn/variance/EMA:0, group2/block5/conv3/bn/gamma:0, group0/block0/conv2/bn/variance/EMA:0, group0/block0/conv1/bn/variance/EMA:0, group2/block1/conv2/bn/mean/EMA:0, group2/block2/conv2/bn/beta:0, group0/block2/conv3/bn/mean/EMA:0\n",
      "\u001b[32m[1014 07:07:26 @sessinit.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the graph, but not found in the dict: fastrcnn/fc6/W, fastrcnn/fc6/b, fastrcnn/fc7/W, fastrcnn/fc7/b, fastrcnn/outputs/box/W, fastrcnn/outputs/box/b, fastrcnn/outputs/class/W, fastrcnn/outputs/class/b, fpn/lateral_1x1_c2/W, fpn/lateral_1x1_c2/b, fpn/lateral_1x1_c3/W, fpn/lateral_1x1_c3/b, fpn/lateral_1x1_c4/W, fpn/lateral_1x1_c4/b, fpn/lateral_1x1_c5/W, fpn/lateral_1x1_c5/b, fpn/posthoc_3x3_p2/W, fpn/posthoc_3x3_p2/b, fpn/posthoc_3x3_p3/W, fpn/posthoc_3x3_p3/b, fpn/posthoc_3x3_p4/W, fpn/posthoc_3x3_p4/b, fpn/posthoc_3x3_p5/W, fpn/posthoc_3x3_p5/b, global_step, learning_rate, maskrcnn/conv/W, maskrcnn/conv/b, maskrcnn/deconv/W, maskrcnn/deconv/b, maskrcnn/fcn0/W, maskrcnn/fcn0/b, maskrcnn/fcn1/W, maskrcnn/fcn1/b, maskrcnn/fcn2/W, maskrcnn/fcn2/b, maskrcnn/fcn3/W, maskrcnn/fcn3/b, rpn/box/W, rpn/box/b, rpn/class/W, rpn/class/b, rpn/conv0/W, rpn/conv0/b\n",
      "\u001b[32m[1014 07:07:26 @sessinit.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the dict, but not found in the graph: linear/W, linear/b\n",
      "\u001b[32m[1014 07:07:26 @sessinit.py:219]\u001b[0m Restoring 265 variables from dict ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 07:07:26.060058 139694008874816 deprecation.py:323] From /home/ec2-user/anaconda3/lib/python3.6/site-packages/tensorpack-0.9.0.1-py3.6.egg/tensorpack/tfutils/varmanip.py:108: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:07:26 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group3/block2/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:26 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group3/block2/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:26 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group3/block2/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:27 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block3/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:28 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block0/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:29 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block4/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:29 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block0/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:29 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/convshortcut/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:29 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block1/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:29 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block1/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:29 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block0/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:30 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block0/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:30 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block3/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:31 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block5/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:31 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block1/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:31 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block3/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:31 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block2/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:32 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group3/block0/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:32 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:32 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable conv0/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:33 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:34 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block2/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:35 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group3/block1/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:36 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group3/block0/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:36 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:36 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block4/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:36 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block2/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:36 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group3/block0/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:37 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block4/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:38 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block2/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:38 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group3/block1/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:38 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block0/convshortcut/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:38 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block2/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:39 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block1/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:40 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block2/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:40 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block3/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:41 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block0/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:42 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block0/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:42 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block0/convshortcut/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:42 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block1/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:42 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block3/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:43 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block1/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:43 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block2/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:44 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block2/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:44 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block1/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:45 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block1/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:45 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block3/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:45 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group3/block0/convshortcut/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:45 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group3/block1/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:45 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block2/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:47 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block1/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:47 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block5/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:48 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group2/block5/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1014 07:07:49 @base.py:244]\u001b[0m Graph Finalized.\n",
      "\u001b[32m[1014 07:07:49 @concurrency.py:40]\u001b[0m Starting EnqueueThread QueueInput/input_queue ...\n",
      "\u001b[32m[1014 07:07:49 @base.py:281]\u001b[0m Start Epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########|1/1[00:25<00:00, 0.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:08:15 @base.py:291]\u001b[0m Epoch 1 (global_step 1) finished, time:25.5 seconds.\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m QueueInput/queue_size: 42\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m boxclass_losses/box_loss: 0.0033458\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m boxclass_losses/label_loss: 4.5305\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m boxclass_losses/label_metrics/accuracy: 0\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m boxclass_losses/label_metrics/false_negative: 0\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m boxclass_losses/label_metrics/fg_accuracy: 0\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m boxclass_losses/num_fg_label: 2\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m learning_rate: 0.003\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m maskrcnn_loss/accuracy: 0.38329\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m maskrcnn_loss/fg_pixel_ratio: 0.66709\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m maskrcnn_loss/maskrcnn_loss: 0.97576\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m maskrcnn_loss/pos_accuracy: 0.10714\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level2: 417\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level3: 73\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level4: 18\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level5: 4\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level2: 0\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level3: 0\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level4: 1\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level5: 1\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m rpn_losses/box_loss: 0.035939\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m rpn_losses/label_loss: 0.68734\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/num_bg: 510\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/num_fg: 2\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/proposal_metrics/best_iou_per_gt: 0.58567\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.3: 1\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.5: 1\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m total_cost: 6.9569\n",
      "\u001b[32m[1014 07:08:15 @monitor.py:469]\u001b[0m wd_cost: 0.72405\n",
      "\u001b[32m[1014 07:08:15 @base.py:295]\u001b[0m Training has finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture cap_out --no-stderr\n",
    "launch_train_with_config(traincfg, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will be stored in the `cap_out` string, but also includes a lot of other junk. To clean it up, use regular expressions and a literal eval.\n",
    "\n",
    "Note that the examples above all follow the format `tensor_something_forward` or `tensor_something_backward` this is to make cleaning up the string easier.\n",
    "\n",
    "The example below assumes the tensors are named something like `tensor_some_tensor_name_backwards`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1014 07:08:31 @input_source.py:178]\u001b[0m EnqueueThread QueueInput/input_queue Exited.\n"
     ]
    }
   ],
   "source": [
    "result = cap_out.stdout.split('[runtime_tensor] tensor_')[1:]\n",
    "result = {i.split('_backward ')[0]: literal_eval(re.sub('\\s+', ', ', i.split('_backward ')[1].strip())) for i in result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['rpn/class/b:0', 'rpn/class/W:0', 'rpn/box/b:0', 'fastrcnn/outputs/class/b:0', 'rpn/conv0/b:0', 'fastrcnn/outputs/box/b:0', 'rpn/box/W:0', 'fastrcnn/fc7/b:0', 'fastrcnn/fc6/b:0', 'maskrcnn/conv/b:0', 'maskrcnn/deconv/b:0', 'maskrcnn/fcn3/b:0', 'fpn/posthoc_3x3_p4/b:0', 'maskrcnn/fcn2/b:0', 'maskrcnn/conv/W:0', 'fastrcnn/outputs/class/W:0', 'maskrcnn/fcn1/b:0', 'maskrcnn/fcn0/b:0', 'maskrcnn/deconv/W:0', 'fastrcnn/outputs/box/W:0', 'fpn/lateral_1x1_c5/b:0', 'rpn/conv0/W:0', 'maskrcnn/fcn1/W:0', 'fpn/lateral_1x1_c4/W:0', 'fpn/posthoc_3x3_p4/W:0', 'maskrcnn/fcn0/W:0', 'fpn/lateral_1x1_c3/b:0', 'fpn/posthoc_3x3_p5/b:0', 'fpn/lateral_1x1_c2/b:0', 'fpn/lateral_1x1_c2/W:0', 'fpn/lateral_1x1_c3/W:0', 'fpn/lateral_1x1_c4/b:0', 'maskrcnn/fcn3/W:0', 'fpn/posthoc_3x3_p3/b:0', 'fpn/posthoc_3x3_p2/b:0', 'fastrcnn/fc7/W:0', 'fpn/posthoc_3x3_p5/W:0', 'maskrcnn/fcn2/W:0', 'fpn/posthoc_3x3_p3/W:0', 'fpn/lateral_1x1_c5/W:0', 'fpn/posthoc_3x3_p2/W:0', 'fastrcnn/fc6/W:0'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.155761719, 0.128295898, 0.165771484]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['rpn/class/b:0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
